{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # this adds to path parent directory in order to import utils file\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from IPython.display import clear_output, Image, display\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import time\n",
    "import pickle\n",
    "import umap\n",
    "import sys, traceback, os\n",
    "## Import custom utils\n",
    "import gmm_utils\n",
    "import hyperopt_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import importlib\n",
    "importlib.reload(gmm_utils)\n",
    "importlib.reload(hyperopt_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runHyperopt(trialsFile, resultsFile, space, max_evals = 2, restart = False):\n",
    "    # Define function to optimise\n",
    "    def evaluateGMM(args):\n",
    "        try:\n",
    "            resultDict, _ = gmm_utils.runGMM(args)\n",
    "            if os.path.isfile(resultsFile):\n",
    "                results = pd.read_pickle(resultsFile)\n",
    "\n",
    "                newDf = pd.DataFrame.from_dict(resultDict, orient='index').T\n",
    "                results = results.append(newDf)\n",
    "            else:\n",
    "                results = pd.DataFrame.from_dict(resultDict, orient='index').T\n",
    "            results.to_pickle(resultsFile)\n",
    "        except:\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            return { 'status' : hyperopt.STATUS_FAIL}\n",
    "\n",
    "        print(f'>> Result: {resultDict[\"_rand_index\"]}')\n",
    "        ret = {\n",
    "            'loss' : -resultDict['_rand_index']\n",
    "            ,'status' : STATUS_OK\n",
    "            ,'eval_time' : time.time()        \n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    trials = hyperopt_utils.getTrials(trialsFile ,restart = restart )\n",
    "    evals_per_epoch = 10\n",
    "    for e in range(len(trials), max_evals, evals_per_epoch):\n",
    "        best = fmin(evaluateGMM\n",
    "                    ,space\n",
    "                    ,algo=tpe.suggest \n",
    "                    ,max_evals= e + evals_per_epoch\n",
    "                    ,trials=trials)\n",
    "        print('Index ', e)\n",
    "        pickle.dump(trials, open(trialsFile, 'wb'))\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain CIDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'brainCIDR'\n",
    "trialsFile = f'{dataset}_gmm_trials.pkl'\n",
    "resultsFile = f'{dataset}_gmm_results.pkl'\n",
    "space = {\n",
    "    'dataset' : dataset\n",
    "    ,'minCellsPerGene':scope.int(hp.quniform('minCellsPerGene', 0, 5, 1))\n",
    "    ,'minGeneDispersion':hp.uniform('minGeneDispersion', 0, 1.5)\n",
    "    ,'log' : hp.choice('log', [True,False])\n",
    "    ,'scaler' : hp.choice('scaler',\n",
    "            ['none','standardScaleGenes', 'standardScaleCells', 'robustScaleGenes', 'robustScaleCells'])\n",
    "    ,'pca_comp' : scope.int(hp.quniform('pca_comp', 3, 300,1))\n",
    "    ,'nb_clusters' : scope.int(hp.quniform('nb_clusters', 3, 15, 1))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke testing\n",
    "# params = {\n",
    "#     'dataset' : 'brainCIDR'\n",
    "#     ,'minCellsPerGene':2\n",
    "#     ,'minGeneDispersion':0.5\n",
    "#     ,'log' : True\n",
    "#     ,'scaler' : 'standardScaleCells'\n",
    "#     ,'pca_comp' : 10\n",
    "#     ,'nb_clusters' : 10\n",
    "# }\n",
    "\n",
    "# params = {'dataset': 'brainCIDR', \n",
    "#           'log': False, \n",
    "#           'minCellsPerGene': 2, \n",
    "#           'minGeneDispersion': 1.3643019957416587, \n",
    "#           'nb_clusters': 9, \n",
    "#           'pca_comp': 246, \n",
    "#           'scaler': 'none'}\n",
    "# gmm_utils.runGMM(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = runHyperopt(trialsFile, resultsFile, space, max_evals = 500, restart = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm_utils.plotBestPrediction(summaryDf, dataset)\n",
    "resultsDf = pd.read_pickle(resultsFile)\n",
    "display(resultsDf.sort_values(by='_rand_index', ascending = False).head(3))\n",
    "gmm_utils.plotCorrelation(resultsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PancreaticIsletCIDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= 'pancreaticIsletCIDR'\n",
    "df, truth = gmm_utils.loadData(dataset)\n",
    "print(df.shape, truth.shape, truth.clusters.unique())\n",
    "trialsFile = f'{dataset}_gmm_trials.pkl'\n",
    "resultsFile = f'{dataset}_gmm_results.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space = {\n",
    "    'dataset' : dataset\n",
    "    ,'minCellsPerGene':scope.int(hp.quniform('minCellsPerGene', 0, 5, 1))\n",
    "    ,'minGeneDispersion':hp.uniform('minGeneDispersion', 0, 1.5)\n",
    "    ,'log' : hp.choice('log', [True,False])\n",
    "    ,'scaler' : hp.choice('scaler',\n",
    "            ['none','standardScaleGenes', 'standardScaleCells', 'robustScaleGenes', 'robustScaleCells'])\n",
    "    ,'pca_comp' : scope.int(hp.quniform('pca_comp', 3, 300,1))\n",
    "    ,'nb_clusters' : scope.int(hp.quniform('nb_clusters', 3, 15, 1))\n",
    "}\n",
    "\n",
    "filename = f'{dataset}_trials.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=hyperopt_utils.getTrials(filename ,restart = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = runHyperopt(filename, space, max_evals = 500, restart = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf = hyperopt_utils.getResultsAsDf(trials, space)\n",
    "summaryDf.sort_values(by='result', ascending =True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf.to_pickle(f'{dataset}_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf = pd.read_pickle(f'{dataset}_df.pkl')\n",
    "gmm_utils.plotBestPrediction(summaryDf, dataset, pca_comp = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= 'deng'\n",
    "trialsFile = f'{dataset}_gmm_trials.pkl'\n",
    "resultsFile = f'{dataset}_gmm_results.pkl'\n",
    "df, truth = gmm_utils.loadData(dataset)\n",
    "umap2D = gmm_utils.getUmap(df, pca_comp = 10)\n",
    "print(df.shape, truth.shape, truth.clusters.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space = {\n",
    "    'dataset' : dataset\n",
    "    ,'minCellsPerGene':scope.int(hp.quniform('minCellsPerGene', 0, 5, 1))\n",
    "    ,'minGeneDispersion':hp.uniform('minGeneDispersion', 0, 1.5)\n",
    "    ,'log' : hp.choice('log', [True,False])\n",
    "    ,'scaler' : hp.choice('scaler',\n",
    "            ['none','standardScaleGenes', 'standardScaleCells', 'robustScaleGenes', 'robustScaleCells'])\n",
    "    ,'pca_comp' : scope.int(hp.quniform('pca_comp', 3, 300,1))\n",
    "    ,'nb_clusters' : scope.int(hp.quniform('nb_clusters', 3, 15, 1))\n",
    "}\n",
    "\n",
    "filename = f'{dataset}_trials.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=hyperopt_utils.getTrials(filename ,restart = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = runHyperopt(filename, space, max_evals = 500, restart = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf = hyperopt_utils.getResultsAsDf(trials, space)\n",
    "summaryDf.sort_values(by='result', ascending =True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf.to_pickle(f'{dataset}_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf = pd.read_pickle(f'{dataset}_df.pkl')\n",
    "gmm_utils.plotBestPrediction(summaryDf, dataset, pca_comp = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestBic, bestAic, bestSil = optimalNbClustersGMM(pc, params['min_clusters'], params['max_clusters'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestBic, bestAic, bestSil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clust in n_clusters:\n",
    "    model = GaussianMixture(n_clust, covariance_type ='full', random_state = 0).fit(pc)\n",
    "    clusters = model.predict(pc)\n",
    "    score = adjusted_rand_score(truth.clusters.tolist(), clusters)\n",
    "    print(f\"For {n_clust} clusters, score : {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianMixture(8, covariance_type ='full', random_state = 0).fit(pc)\n",
    "clusters = model.predict(pc)\n",
    "score = adjusted_rand_score(truth.clusters.tolist(), clusters)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth.clusters.value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'brainCIDR'\n",
    "df, truth = gmm_utils.loadData(dataset)\n",
    "umap2D = gmm_utils.getUmap(df, pca_comp = 10)\n",
    "print(df.shape, truth.shape, truth.clusters.unique())\n",
    "params={}\n",
    "params['dataset'] = 'brainCIDR'\n",
    "params['minCellsPerGene'] = 0\n",
    "params['minGeneDispersion'] = 0\n",
    "params['log'] = True # True, False\n",
    "params['scaler'] = 'standardScaleCells'# \n",
    "params['pca_comp'] = 10 #range (3, 300)\n",
    "params['nb_clusters'] =8 #3 -15\n",
    "gmm_utils.run(params);"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
